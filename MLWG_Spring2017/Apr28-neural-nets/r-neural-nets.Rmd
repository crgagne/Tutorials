---
title: "Neural networks in R"
output:
  html_document: default
  html_notebook: default
---

Topics to cover:

* Background
* Single-layer networks
* Multi-layer networks
* Possibly more


Before we dig in, we will install the R packages we'll be using.

**R packages**
```{r}
# List of packages we will use.
packages = c("MASS", "nnet", "h2o", "devtools")

# Try to load each package and save the result.
success = sapply(packages, require, character.only = T, quietly = T)

# Check if any packages still need to be installed.
if (sum(!success) > 0) {
  # Install any needed packages.
  install.packages(packages[!success])
  
  # Load the newly installed packages.
  sapply(packages[!success], require, character.only = T, quietly = T)
}

github_packages = c(
  # Chris's tools package for plotting the SuperLearner.
  "ck37/ck37r",
  # Use more up-to-date SuperLearner from github.
  "ecpolley/SuperLearner")

devtools::install_github(github_packages)

# Also install mxnet for potential usage.
# This unfortunately is Mac/Windows only; probably will not work for Linux.
# Actually not working for Mac either.
if (F) {
  # Skip this for now.
  install.packages("drat", repos="https://cran.rstudio.com")
  drat:::addRepo("dmlc")
  install.packages("mxnet")
}

# Clean up variables.
rm(packages, success, github_packages)
```

# Background



# Software packages



# Data preparation

```{r}
data(Boston, package = "MASS")

# Remove our outcome variable from the covariate list.
X_df = Boston[, -14]

# Convert X from a dataframe to a matrix.
X_mat = model.matrix(~ .,  data = X_df)

# Notice the extra intercept column added by model.matrix.
colnames(X_mat)
                 
#  Remove extra intercept term.
X_mat = X_mat[, -1]

# Regression (continuous) version of our outcome variable.
Y_reg = Boston$medv

# Review outcome distribution.
summary(Y_reg)

# Classification (binary) version of our outcome variable.
Y_class = as.factor(as.numeric(Boston$medv > 23))

# Review outcome distribution.
table(Y_class)
prop.table(table(Y_class))

```

# Single-layer neural network


Quick classification example

```{r}
library(nnet)

# Classification

# Set seed because weights are initialized randomly.
set.seed(1)

# X can be a dataframe or matrix.
# If Y is a factor we need to use this formula notation.
fit = nnet(Y_class ~ X_mat, size = 2, decay = 5e-4, maxit = 200)

# Review our neural network fit.
fit

# Predict back to our original data.
pred = predict(fit, X_mat)

# Review predictions.
summary(pred)
```

Quick regression example

```{r}
library(nnet)

# Set seed because weights are initialized randomly.
set.seed(1)

# Again, X can be a dataframe or matrix.
fit = nnet(Y_reg ~ X_mat, size = 2, decay = 5e-4, maxit = 200,
           # Enable linear output to support regression.
           linout = T)

# Challenge: try with linout = F (the default) and see what happens.

# Review our neural network fit.
fit

# Predict back to our original data.
pred = predict(fit, X_mat)

# Review predictions.
summary(pred)

# Calculate mean-squared error (MSE).
mean((pred - Y_reg)^2)

# And root mean squared error (RMSE).
sqrt(mean((pred - Y_reg)^2))
```

# SuperLearner optimization

These challenges can be done in pairs/groups to make it easier.

Challenge 1: use SL.nnet wrapper to estimate performance of the neural network.

Challenge 2: use create.Learner() to test 2, 3, 4, or 5 hidden units and create a weighted average ensemble.

# Multi-layer neural network

Challenge: use h2o to design this.
