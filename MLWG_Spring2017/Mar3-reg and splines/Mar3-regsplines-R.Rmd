---
title: "regression and splines (working version)" 
author: "Evan Muzzall"
date: "3/3/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##1. Package installations
```{r}
if (FALSE) {
  install.packages("Zelig")
  devtools::install_github("ck37/ck37r")
}

library(splines) # call the base "splines"
library(Zelig) # this contains the "macro" dataset we will use 
```

##2. Goals
First, we will walk through linear regression, polynomial regression, polynomial splines, and smoothing splines using an incredibly simple example.  

Then, we will attempt to apply what we learned and do the same thing using the "macro" dataset from the "Zelig" package to see a more real life example.   

##3. Simple linear regression
Simple linear regression uses a single predictor/input/independent variable (X) to predict one target/outcome/response/dependent variably (Y). Ideally, we want to find the best estimates for B0 (intercept) and B1 (slope) that minimize the error terms when using X to predict Y.  
```{r}
## generate toy predictors and responses
X <- c(2, 4, 8, 12, 18, 20)
Y <- c(1, 3, 5, 9, 19, 21)

## calculate means
mean(X)
mean(Y)

## calculate error for each observation
X-mean(X)
Y-mean(Y)

## plot the data
plot(x=X, y=Y, main="example")

## estimate B1 coefficient (slope)
B1 <- sum((X-mean(X)) * (Y-mean(Y))) / sum((X-mean(X))^2)
B1

## now estimate B0 coefficient (intercept)
B0 <- mean(Y) - (B1 * mean(X))
B0

## plot the abline
abline(B0, B1, col="black", lwd=2)
legend("topleft", inset=.0, c("linear"), lty=1, lwd=2, col="black", cex=.75)

## generate predicted values by plugging in our X values to the equation: 
Y_hat <- B0 + B1 * X
Y_hat

## calculate root mean sqaure error (RMSE) for our predictions. First, calculatte the error for each observation by subracting it from the predicted value:
Y_err <- Y_hat - Y
Y_err

## then, calculate the square of each of these errors:
Y_err_sq <- Y_err^2
Y_err_sq

## sum these values
sum_squared_err <- sum(Y_err_sq)
sum_squared_err

## divide by n and take square root to produce the RMSE:
RMSE <- sqrt(sum_squared_err / length(Y))
RMSE

## sanity check
RMSE == sqrt(sum((Y_hat - Y)^2) / length(Y))

## double sanity check
## fit the model using the lm function in R:
lm_toy <- lm(Y ~ X)
lm_toy
B1
B0
summary(lm_toy)

## is our B1 the same as the slope generated by "lm" in R?
round(B1, digits=5) == round(lm_toy$coefficients[2], digits=5)

## is our B0 the same as the intercept generated by "lm" in R?
round(B0, digits=5) == round(lm_toy$coefficients[1], digits=5)
```

##4. Polynomial regression
However, it is not always advisable to assume linear relationships within data. Although linear models are flexible, they might not best express the relationships between your predictor and response variable. Thus, your resulting p-values might not accurately reflect the null hypothesis that the variables are not associated. 

Polynomial regression raises the original predictor variable to the n^th^ degree. These scalars act as a means to increase the fit of the model by assuming the point distributions are more parabolic shaped than linear.  

We will fit a 3^rd^ degree (cubic) polynomial so that our series of equations looks like this:
Y ~ X  
Y ~ X^2  
Y ~ X^3  
(plus the error term)  

The major drawback of polynomial regression however, is that the function is fit to the global feature space. That is, a single polynomial function is fit in an attempt to represent all data points. However, since this is essentially a linear regression model, coefficients can still be estimated using least squares. 
```{r}
poly1 <- lm(Y ~ X + I(X^2) + I(X^3))
poly1
summary(poly1)
## so, what is really happening here? 

## imagine we take our X variable and create a new column in a data frame that would look like this:
X2 <- X^2
X3 <- X^3

toy_df <- data.frame(Y, X, X2, X3)
toy_df

## the "poly" function produces the same results (when raw=TRUE)
poly2 <- lm(Y ~ poly(X, 3, raw=TRUE))
poly2
summary(poly2)
lines(X ~ fitted(poly2), lty=2, lwd=2, col="red")
legend("topleft", inset=.0, c("linear", "poly 3"), lty=c(1,2), lwd=2, col=c("black","red"), cex=.75)

## sanity check
poly1$coefficients == poly2$coefficients
```

##5. Polynomial splines
Polynomial splines are [piecewise polynomial functions](https://www.khanacademy.org/math/algebra/algebra-functions/piecewise-functions/v/piecewise-function-example) that form smoothed curved shapes at their junctions (called "knots"). The X predictor is divided into K regions, and a polynomial function is fit to the data within each region. This allows for greater flexibility than linear or polynomial fits. This is a k^th^ order spline where coefficients can be estimated by least squares. Derivatives are fit between each knot, and each imposed constraint releases a degree of freedom, thus smoothing the polynomial fits.  As such, polynomial splines are generally more flexible fits than polynomial regression models.

"poly spline" (basis-spline) is the function that allows for continuous joins at the spline knots. It is the matrix that contains the information of the piecewise polynomial functions used to fit the spline.  
```{r}
## create xy data frame using our X and Y variables
xy <- data.frame(X, Y)
xy

bs(xy$X, df=3)
summary(ps1 <- lm(Y ~ bs(X, df=3, knots=8), data = xy))

## example of "safe" prediction (see Chambers JM, Hastie TJ (editors). 1992. Statistical Models in S. Pacific Grove, CA:  Wadsworth and Brooks/Cole. p 288-289 for "smart" versus "safe" prediction)
summary(xy$X)
X_pred <- seq(min(xy$X), max(xy$X), len = 200)
lines(X_pred, predict(ps1, data.frame(X=X_pred)), lty=3, lwd=2, col="green")
legend("topleft", inset=.0, c("linear", "poly 3", "poly spline"), lty=c(1,2,3), lwd=2, col=c("black","red", "green"), cex=.75)
```

Consider using "SuperLearner" to find optimal number of knots!  

##6. Smoothing splines
Smoothing splines are similar to the poly splines above, except they produce knots at each data point and coefficients of the estimated function are shrunk via regularization, thus helping prevent overfitting.  

The goal is to minimize loss+penalty in addition to a small residual sum of squares - this forms the smoothing spline. However, by treating each X observation as a data point we might be concerned with getting an overfit model. Thus it is useful to talk about degrees of freedom in terms of their _effective degrees of freedom_, or a general representation of the flexibility of the smoothing spline of shrunken degrees of freedom.  
Also, intervals are allowed to overlap. 
```{r}
smooth1 <- smooth.spline(y=Y, x=X, cv=FALSE, keep.data=TRUE, spar=NULL, penalty=1)
smooth1
names(smooth1)
smooth1$data

lines(smooth1, col="blue", lty=4)
legend("topleft", inset=.0, c("linear", "poly 3", "poly spline", "smooth1"), lty=c(1,2,3,4), lwd=2, col=c("black","red", "green", "blue"), cex=.75)
```


##7. Repeat with the "macro" data from the "Zelig" package
```{r}
data(macro, package = "Zelig")
macro_lm <- lm(gdp ~ unem, data=macro)
macro_lm
summary(macro_lm)

## plot residuals
hist(macro_lm$residuals)

## plot it
plot(macro$unem, macro$gdp, col="gray80", 
     main="'macro' gdp ~ unem",
     xlab="unem deficit",
     ylab=)

abline(macro_lm$coefficients[1], macro_lm$coefficients[2],
       lwd=2, col="black")
legend("topleft", inset=.0, c("linear"), lty=1, lwd=2, col="black", cex=.75)

## generate predicted values by plugging in our X values. 
macro_pred <- predict(macro_lm, macro)

## or, this is the same as our formula way from the toy example:
macro_pred <- macro_lm$coefficients[1] + macro_lm$coefficients[2] * macro$unem

## check MSE on the predicted values
MSE <- mean((macro$gdp - macro_pred)^2)
MSE
```

##9. Cubic polynomial regression
```{r}
poly_macro <- lm(gdp ~ poly(unem, 3, raw=TRUE), data=macro)
poly_macro
summary(poly_macro)

unem_lims <- range(macro$unem)
unem_grid <- seq(from=unem_lims[1], to=unem_lims[2])
poly_preds <- predict(poly_macro, newdata=list(unem=unem_grid), se=TRUE)

lines(unem_grid, poly_preds$fit, lty=2, lwd=2, col="red")
legend("topleft", inset=.0, c("linear", "ploy 4"), lty=c(1,2), lwd=2, col=c("black", "red"), cex=.75)
```

##10. poly spline
```{r}
ps2 <- lm(gdp ~ bs(unem, df=55), data = macro)
summary(ps2)

## example of "safe" prediction
summary(macro$unem)
X_pred2 <- seq(min(macro$unem), max(macro$unem), len = 200)
lines(X_pred2, predict(ps2, data.frame(unem=X_pred2)), lty=3, lwd=2, col="green")
legend("topleft", inset=.0, c("linear", "poly 3", "poly spline"), lty=c(1,2,3), lwd=2, col=c("black","red", "green"), cex=.75)
```

##11. Smoothing spline
```{r}
smooth2 <- smooth.spline(y=macro$gdp, x=macro$unem, cv=FALSE)
smooth2

lines(smooth2, col="blue", lty=4)
legend("topleft", inset=.0, c("linear", "poly 3", "poly spline", "smooth1"), lty=c(1,2,3,4), lwd=2, col=c("black","red", "green", "blue"), cex=.75)
```

## Acknowledgements
James G, Witten D, Hastie T, Tibshirani R. 2015. An Introduction to Statistical Learning: With Applications in R (6th printing). New York: Springer. 
