{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with DTs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the famous *iris* dataset, dealing with plant classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look inside of it to see what datatypes scikit-learn wants, and how their sample dataset is formatted, so that we can prepare our own datasets later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'DESCR', 'target', 'feature_names', 'target_names'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the data is in dictionary format, and we can access the data and labels by indexing certain keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Iris Plants Database\\n====================\\n\\nNotes\\n-----\\nData Set Characteristics:\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThis is a copy of UCI ML iris datasets.\\nhttp://archive.ics.uci.edu/ml/datasets/Iris\\n\\nThe famous Iris database, first used by Sir R.A Fisher\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\nReferences\\n----------\\n   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.DESCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what are the features, and what are we predicting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "4\n",
      "\n",
      "['setosa' 'versicolor' 'virginica']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(iris.feature_names)\n",
    "print(len(iris.feature_names))\n",
    "print()\n",
    "print(iris.target_names)\n",
    "print(len(iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are using 4 features for each observation, trying to classfiy each observation into one of three categories, using only those 4 features. How are these input features formatted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 5.1,  3.5,  1.4,  0.2],\n",
       "       [ 4.9,  3. ,  1.4,  0.2],\n",
       "       [ 4.7,  3.2,  1.3,  0.2],\n",
       "       [ 4.6,  3.1,  1.5,  0.2],\n",
       "       [ 5. ,  3.6,  1.4,  0.2],\n",
       "       [ 5.4,  3.9,  1.7,  0.4],\n",
       "       [ 4.6,  3.4,  1.4,  0.3],\n",
       "       [ 5. ,  3.4,  1.5,  0.2],\n",
       "       [ 4.4,  2.9,  1.4,  0.2],\n",
       "       [ 4.9,  3.1,  1.5,  0.1],\n",
       "       [ 5.4,  3.7,  1.5,  0.2],\n",
       "       [ 4.8,  3.4,  1.6,  0.2],\n",
       "       [ 4.8,  3. ,  1.4,  0.1],\n",
       "       [ 4.3,  3. ,  1.1,  0.1],\n",
       "       [ 5.8,  4. ,  1.2,  0.2],\n",
       "       [ 5.7,  4.4,  1.5,  0.4],\n",
       "       [ 5.4,  3.9,  1.3,  0.4],\n",
       "       [ 5.1,  3.5,  1.4,  0.3],\n",
       "       [ 5.7,  3.8,  1.7,  0.3],\n",
       "       [ 5.1,  3.8,  1.5,  0.3],\n",
       "       [ 5.4,  3.4,  1.7,  0.2],\n",
       "       [ 5.1,  3.7,  1.5,  0.4],\n",
       "       [ 4.6,  3.6,  1. ,  0.2],\n",
       "       [ 5.1,  3.3,  1.7,  0.5],\n",
       "       [ 4.8,  3.4,  1.9,  0.2],\n",
       "       [ 5. ,  3. ,  1.6,  0.2],\n",
       "       [ 5. ,  3.4,  1.6,  0.4],\n",
       "       [ 5.2,  3.5,  1.5,  0.2],\n",
       "       [ 5.2,  3.4,  1.4,  0.2],\n",
       "       [ 4.7,  3.2,  1.6,  0.2],\n",
       "       [ 4.8,  3.1,  1.6,  0.2],\n",
       "       [ 5.4,  3.4,  1.5,  0.4],\n",
       "       [ 5.2,  4.1,  1.5,  0.1],\n",
       "       [ 5.5,  4.2,  1.4,  0.2],\n",
       "       [ 4.9,  3.1,  1.5,  0.1],\n",
       "       [ 5. ,  3.2,  1.2,  0.2],\n",
       "       [ 5.5,  3.5,  1.3,  0.2],\n",
       "       [ 4.9,  3.1,  1.5,  0.1],\n",
       "       [ 4.4,  3. ,  1.3,  0.2],\n",
       "       [ 5.1,  3.4,  1.5,  0.2],\n",
       "       [ 5. ,  3.5,  1.3,  0.3],\n",
       "       [ 4.5,  2.3,  1.3,  0.3],\n",
       "       [ 4.4,  3.2,  1.3,  0.2],\n",
       "       [ 5. ,  3.5,  1.6,  0.6],\n",
       "       [ 5.1,  3.8,  1.9,  0.4],\n",
       "       [ 4.8,  3. ,  1.4,  0.3],\n",
       "       [ 5.1,  3.8,  1.6,  0.2],\n",
       "       [ 4.6,  3.2,  1.4,  0.2],\n",
       "       [ 5.3,  3.7,  1.5,  0.2],\n",
       "       [ 5. ,  3.3,  1.4,  0.2],\n",
       "       [ 7. ,  3.2,  4.7,  1.4],\n",
       "       [ 6.4,  3.2,  4.5,  1.5],\n",
       "       [ 6.9,  3.1,  4.9,  1.5],\n",
       "       [ 5.5,  2.3,  4. ,  1.3],\n",
       "       [ 6.5,  2.8,  4.6,  1.5],\n",
       "       [ 5.7,  2.8,  4.5,  1.3],\n",
       "       [ 6.3,  3.3,  4.7,  1.6],\n",
       "       [ 4.9,  2.4,  3.3,  1. ],\n",
       "       [ 6.6,  2.9,  4.6,  1.3],\n",
       "       [ 5.2,  2.7,  3.9,  1.4],\n",
       "       [ 5. ,  2. ,  3.5,  1. ],\n",
       "       [ 5.9,  3. ,  4.2,  1.5],\n",
       "       [ 6. ,  2.2,  4. ,  1. ],\n",
       "       [ 6.1,  2.9,  4.7,  1.4],\n",
       "       [ 5.6,  2.9,  3.6,  1.3],\n",
       "       [ 6.7,  3.1,  4.4,  1.4],\n",
       "       [ 5.6,  3. ,  4.5,  1.5],\n",
       "       [ 5.8,  2.7,  4.1,  1. ],\n",
       "       [ 6.2,  2.2,  4.5,  1.5],\n",
       "       [ 5.6,  2.5,  3.9,  1.1],\n",
       "       [ 5.9,  3.2,  4.8,  1.8],\n",
       "       [ 6.1,  2.8,  4. ,  1.3],\n",
       "       [ 6.3,  2.5,  4.9,  1.5],\n",
       "       [ 6.1,  2.8,  4.7,  1.2],\n",
       "       [ 6.4,  2.9,  4.3,  1.3],\n",
       "       [ 6.6,  3. ,  4.4,  1.4],\n",
       "       [ 6.8,  2.8,  4.8,  1.4],\n",
       "       [ 6.7,  3. ,  5. ,  1.7],\n",
       "       [ 6. ,  2.9,  4.5,  1.5],\n",
       "       [ 5.7,  2.6,  3.5,  1. ],\n",
       "       [ 5.5,  2.4,  3.8,  1.1],\n",
       "       [ 5.5,  2.4,  3.7,  1. ],\n",
       "       [ 5.8,  2.7,  3.9,  1.2],\n",
       "       [ 6. ,  2.7,  5.1,  1.6],\n",
       "       [ 5.4,  3. ,  4.5,  1.5],\n",
       "       [ 6. ,  3.4,  4.5,  1.6],\n",
       "       [ 6.7,  3.1,  4.7,  1.5],\n",
       "       [ 6.3,  2.3,  4.4,  1.3],\n",
       "       [ 5.6,  3. ,  4.1,  1.3],\n",
       "       [ 5.5,  2.5,  4. ,  1.3],\n",
       "       [ 5.5,  2.6,  4.4,  1.2],\n",
       "       [ 6.1,  3. ,  4.6,  1.4],\n",
       "       [ 5.8,  2.6,  4. ,  1.2],\n",
       "       [ 5. ,  2.3,  3.3,  1. ],\n",
       "       [ 5.6,  2.7,  4.2,  1.3],\n",
       "       [ 5.7,  3. ,  4.2,  1.2],\n",
       "       [ 5.7,  2.9,  4.2,  1.3],\n",
       "       [ 6.2,  2.9,  4.3,  1.3],\n",
       "       [ 5.1,  2.5,  3. ,  1.1],\n",
       "       [ 5.7,  2.8,  4.1,  1.3],\n",
       "       [ 6.3,  3.3,  6. ,  2.5],\n",
       "       [ 5.8,  2.7,  5.1,  1.9],\n",
       "       [ 7.1,  3. ,  5.9,  2.1],\n",
       "       [ 6.3,  2.9,  5.6,  1.8],\n",
       "       [ 6.5,  3. ,  5.8,  2.2],\n",
       "       [ 7.6,  3. ,  6.6,  2.1],\n",
       "       [ 4.9,  2.5,  4.5,  1.7],\n",
       "       [ 7.3,  2.9,  6.3,  1.8],\n",
       "       [ 6.7,  2.5,  5.8,  1.8],\n",
       "       [ 7.2,  3.6,  6.1,  2.5],\n",
       "       [ 6.5,  3.2,  5.1,  2. ],\n",
       "       [ 6.4,  2.7,  5.3,  1.9],\n",
       "       [ 6.8,  3. ,  5.5,  2.1],\n",
       "       [ 5.7,  2.5,  5. ,  2. ],\n",
       "       [ 5.8,  2.8,  5.1,  2.4],\n",
       "       [ 6.4,  3.2,  5.3,  2.3],\n",
       "       [ 6.5,  3. ,  5.5,  1.8],\n",
       "       [ 7.7,  3.8,  6.7,  2.2],\n",
       "       [ 7.7,  2.6,  6.9,  2.3],\n",
       "       [ 6. ,  2.2,  5. ,  1.5],\n",
       "       [ 6.9,  3.2,  5.7,  2.3],\n",
       "       [ 5.6,  2.8,  4.9,  2. ],\n",
       "       [ 7.7,  2.8,  6.7,  2. ],\n",
       "       [ 6.3,  2.7,  4.9,  1.8],\n",
       "       [ 6.7,  3.3,  5.7,  2.1],\n",
       "       [ 7.2,  3.2,  6. ,  1.8],\n",
       "       [ 6.2,  2.8,  4.8,  1.8],\n",
       "       [ 6.1,  3. ,  4.9,  1.8],\n",
       "       [ 6.4,  2.8,  5.6,  2.1],\n",
       "       [ 7.2,  3. ,  5.8,  1.6],\n",
       "       [ 7.4,  2.8,  6.1,  1.9],\n",
       "       [ 7.9,  3.8,  6.4,  2. ],\n",
       "       [ 6.4,  2.8,  5.6,  2.2],\n",
       "       [ 6.3,  2.8,  5.1,  1.5],\n",
       "       [ 6.1,  2.6,  5.6,  1.4],\n",
       "       [ 7.7,  3. ,  6.1,  2.3],\n",
       "       [ 6.3,  3.4,  5.6,  2.4],\n",
       "       [ 6.4,  3.1,  5.5,  1.8],\n",
       "       [ 6. ,  3. ,  4.8,  1.8],\n",
       "       [ 6.9,  3.1,  5.4,  2.1],\n",
       "       [ 6.7,  3.1,  5.6,  2.4],\n",
       "       [ 6.9,  3.1,  5.1,  2.3],\n",
       "       [ 5.8,  2.7,  5.1,  1.9],\n",
       "       [ 6.8,  3.2,  5.9,  2.3],\n",
       "       [ 6.7,  3.3,  5.7,  2.5],\n",
       "       [ 6.7,  3. ,  5.2,  2.3],\n",
       "       [ 6.3,  2.5,  5. ,  1.9],\n",
       "       [ 6.5,  3. ,  5.2,  2. ],\n",
       "       [ 6.2,  3.4,  5.4,  2.3],\n",
       "       [ 5.9,  3. ,  5.1,  1.8]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(iris.data))\n",
    "print(type(iris.data))\n",
    "iris.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a large numpy array of length 150, one for each observation, and each observation has its own numpy array of length 4, one for each feature. Each inner array *must* lineup with the order of the variables *and* all other arrays. **ORDER MATTERS**.\n",
    "\n",
    "What about the prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(iris.target))\n",
    "print(type(iris.target))\n",
    "iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we have 150 observations, but *no* sub arrays. The target data is one dimension. Order matters here as well, they should correspond to the feature indices in the data array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the data into training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chench/anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target,\n",
    "                                                    train_size=0.75, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the train/test split, scikit-learn makes the rest of the process easy. We just have to decide on our parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.973684210526\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "dt_classifier = tree.DecisionTreeClassifier(criterion='gini',  # or 'entropy' for information gain\n",
    "                       splitter='best',  # or 'random' for random best split\n",
    "                       max_depth=None,  # how deep tree nodes can go\n",
    "                       min_samples_split=2,  # samples needed to split node\n",
    "                       min_samples_leaf=1,  # samples needed for a leaf\n",
    "                       min_weight_fraction_leaf=0.0,  # weight of samples needed for a node\n",
    "                       max_features=None,  # number of features to look for when splitting\n",
    "                       max_leaf_nodes=None,  # max nodes\n",
    "                       min_impurity_split=1e-07)  # early stopping\n",
    "\n",
    "model = dt_classifier.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 4)\t1\n",
      "  (3, 0)\t1\n",
      "  (3, 1)\t1\n",
      "  (4, 0)\t1\n",
      "  (4, 2)\t1\n",
      "  (4, 10)\t1\n",
      "  (4, 14)\t1\n",
      "  (5, 0)\t1\n",
      "  (5, 2)\t1\n",
      "  (5, 3)\t1\n",
      "  (5, 4)\t1\n",
      "  (6, 0)\t1\n",
      "  (6, 2)\t1\n",
      "  (6, 10)\t1\n",
      "  (6, 14)\t1\n",
      "  (7, 0)\t1\n",
      "  :\t:\n",
      "  (30, 4)\t1\n",
      "  (31, 0)\t1\n",
      "  (31, 2)\t1\n",
      "  (31, 10)\t1\n",
      "  (31, 14)\t1\n",
      "  (32, 0)\t1\n",
      "  (32, 2)\t1\n",
      "  (32, 3)\t1\n",
      "  (32, 4)\t1\n",
      "  (33, 0)\t1\n",
      "  (33, 2)\t1\n",
      "  (33, 10)\t1\n",
      "  (33, 14)\t1\n",
      "  (34, 0)\t1\n",
      "  (34, 2)\t1\n",
      "  (34, 10)\t1\n",
      "  (34, 14)\t1\n",
      "  (35, 0)\t1\n",
      "  (35, 1)\t1\n",
      "  (36, 0)\t1\n",
      "  (36, 2)\t1\n",
      "  (36, 3)\t1\n",
      "  (36, 4)\t1\n",
      "  (37, 0)\t1\n",
      "  (37, 1)\t1\n"
     ]
    }
   ],
   "source": [
    "print(model.decision_path(X_test)) # Return a node indicator matrix where non zero elements indicates that the samples goes through the nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_leaf': 1, 'min_samples_split': 3}\n",
      "0.9375\n",
      "0.973684210526\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'min_samples_split': range(2,10),\n",
    "              'min_samples_leaf': range(1,10)}\n",
    "\n",
    "model_c = GridSearchCV(tree.DecisionTreeClassifier(), param_grid)\n",
    "model_c.fit(X_train, y_train)\n",
    "\n",
    "best_index = np.argmax(model_c.cv_results_[\"mean_test_score\"])\n",
    "\n",
    "print(model_c.cv_results_[\"params\"][best_index])\n",
    "print(max(model_c.cv_results_[\"mean_test_score\"]))\n",
    "print(model_c.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression with DTs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration, we will use a boston housing dataset, which comes with scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to follow along in other tutorials in the scikit-learn documentation, you will need to know the data structures used as inputs to the models. Let'see what's in the boston dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'DESCR', 'target', 'feature_names'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The description will tell us more about the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Boston House Prices dataset\\n===========================\\n\\nNotes\\n------\\nData Set Characteristics:  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive\\n    \\n    :Median Value (attribute 14) is usually the target\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttp://archive.ics.uci.edu/ml/datasets/Housing\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n**References**\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.DESCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are working on predicitng median value of a home from 506 observations, and 13 covariates including crime rate, lot size, industry/commercial proportion, presence of the Charles River, nitric oxide concentration, rooms per dwelling, units built before 1940, distance to employment centers, access to highways, tax rate, school proxy, black population, and status. To get the variable names we can ask for them in the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "\n",
      "<class 'numpy.ndarray'>\n",
      "\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "print(boston.feature_names)\n",
    "print()\n",
    "print(type(boston.feature_names))\n",
    "print()\n",
    "print(len(boston.feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the input is a numpy array of strings for the variable labels. To get the variable data, we ask the dictionary for the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.32000000e-03   1.80000000e+01   2.31000000e+00 ...,   1.53000000e+01\n",
      "    3.96900000e+02   4.98000000e+00]\n",
      " [  2.73100000e-02   0.00000000e+00   7.07000000e+00 ...,   1.78000000e+01\n",
      "    3.96900000e+02   9.14000000e+00]\n",
      " [  2.72900000e-02   0.00000000e+00   7.07000000e+00 ...,   1.78000000e+01\n",
      "    3.92830000e+02   4.03000000e+00]\n",
      " ..., \n",
      " [  6.07600000e-02   0.00000000e+00   1.19300000e+01 ...,   2.10000000e+01\n",
      "    3.96900000e+02   5.64000000e+00]\n",
      " [  1.09590000e-01   0.00000000e+00   1.19300000e+01 ...,   2.10000000e+01\n",
      "    3.93450000e+02   6.48000000e+00]\n",
      " [  4.74100000e-02   0.00000000e+00   1.19300000e+01 ...,   2.10000000e+01\n",
      "    3.96900000e+02   7.88000000e+00]]\n",
      "\n",
      "<class 'numpy.ndarray'>\n",
      "\n",
      "506\n"
     ]
    }
   ],
   "source": [
    "print(boston.data)\n",
    "print()\n",
    "print(type(boston.data))\n",
    "print()\n",
    "print(len(boston.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is a numpy array, inside of which there is a separate array for each observation (all 506 for each hous, *not* 13 for each variable). Each inner array *must* lineup with the order of the variables *and* all other arrays. **ORDER MATTERS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target, or *y* is accessed in the dictionary as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 24.   21.6  34.7  33.4  36.2  28.7  22.9  27.1  16.5  18.9  15.   18.9\n",
      "  21.7  20.4  18.2  19.9  23.1  17.5  20.2  18.2  13.6  19.6  15.2  14.5\n",
      "  15.6  13.9  16.6  14.8  18.4  21.   12.7  14.5  13.2  13.1  13.5  18.9\n",
      "  20.   21.   24.7  30.8  34.9  26.6  25.3  24.7  21.2  19.3  20.   16.6\n",
      "  14.4  19.4  19.7  20.5  25.   23.4  18.9  35.4  24.7  31.6  23.3  19.6\n",
      "  18.7  16.   22.2  25.   33.   23.5  19.4  22.   17.4  20.9  24.2  21.7\n",
      "  22.8  23.4  24.1  21.4  20.   20.8  21.2  20.3  28.   23.9  24.8  22.9\n",
      "  23.9  26.6  22.5  22.2  23.6  28.7  22.6  22.   22.9  25.   20.6  28.4\n",
      "  21.4  38.7  43.8  33.2  27.5  26.5  18.6  19.3  20.1  19.5  19.5  20.4\n",
      "  19.8  19.4  21.7  22.8  18.8  18.7  18.5  18.3  21.2  19.2  20.4  19.3\n",
      "  22.   20.3  20.5  17.3  18.8  21.4  15.7  16.2  18.   14.3  19.2  19.6\n",
      "  23.   18.4  15.6  18.1  17.4  17.1  13.3  17.8  14.   14.4  13.4  15.6\n",
      "  11.8  13.8  15.6  14.6  17.8  15.4  21.5  19.6  15.3  19.4  17.   15.6\n",
      "  13.1  41.3  24.3  23.3  27.   50.   50.   50.   22.7  25.   50.   23.8\n",
      "  23.8  22.3  17.4  19.1  23.1  23.6  22.6  29.4  23.2  24.6  29.9  37.2\n",
      "  39.8  36.2  37.9  32.5  26.4  29.6  50.   32.   29.8  34.9  37.   30.5\n",
      "  36.4  31.1  29.1  50.   33.3  30.3  34.6  34.9  32.9  24.1  42.3  48.5\n",
      "  50.   22.6  24.4  22.5  24.4  20.   21.7  19.3  22.4  28.1  23.7  25.\n",
      "  23.3  28.7  21.5  23.   26.7  21.7  27.5  30.1  44.8  50.   37.6  31.6\n",
      "  46.7  31.5  24.3  31.7  41.7  48.3  29.   24.   25.1  31.5  23.7  23.3\n",
      "  22.   20.1  22.2  23.7  17.6  18.5  24.3  20.5  24.5  26.2  24.4  24.8\n",
      "  29.6  42.8  21.9  20.9  44.   50.   36.   30.1  33.8  43.1  48.8  31.\n",
      "  36.5  22.8  30.7  50.   43.5  20.7  21.1  25.2  24.4  35.2  32.4  32.\n",
      "  33.2  33.1  29.1  35.1  45.4  35.4  46.   50.   32.2  22.   20.1  23.2\n",
      "  22.3  24.8  28.5  37.3  27.9  23.9  21.7  28.6  27.1  20.3  22.5  29.\n",
      "  24.8  22.   26.4  33.1  36.1  28.4  33.4  28.2  22.8  20.3  16.1  22.1\n",
      "  19.4  21.6  23.8  16.2  17.8  19.8  23.1  21.   23.8  23.1  20.4  18.5\n",
      "  25.   24.6  23.   22.2  19.3  22.6  19.8  17.1  19.4  22.2  20.7  21.1\n",
      "  19.5  18.5  20.6  19.   18.7  32.7  16.5  23.9  31.2  17.5  17.2  23.1\n",
      "  24.5  26.6  22.9  24.1  18.6  30.1  18.2  20.6  17.8  21.7  22.7  22.6\n",
      "  25.   19.9  20.8  16.8  21.9  27.5  21.9  23.1  50.   50.   50.   50.\n",
      "  50.   13.8  13.8  15.   13.9  13.3  13.1  10.2  10.4  10.9  11.3  12.3\n",
      "   8.8   7.2  10.5   7.4  10.2  11.5  15.1  23.2   9.7  13.8  12.7  13.1\n",
      "  12.5   8.5   5.    6.3   5.6   7.2  12.1   8.3   8.5   5.   11.9  27.9\n",
      "  17.2  27.5  15.   17.2  17.9  16.3   7.    7.2   7.5  10.4   8.8   8.4\n",
      "  16.7  14.2  20.8  13.4  11.7   8.3  10.2  10.9  11.    9.5  14.5  14.1\n",
      "  16.1  14.3  11.7  13.4   9.6   8.7   8.4  12.8  10.5  17.1  18.4  15.4\n",
      "  10.8  11.8  14.9  12.6  14.1  13.   13.4  15.2  16.1  17.8  14.9  14.1\n",
      "  12.7  13.5  14.9  20.   16.4  17.7  19.5  20.2  21.4  19.9  19.   19.1\n",
      "  19.1  20.1  19.9  19.6  23.2  29.8  13.8  13.3  16.7  12.   14.6  21.4\n",
      "  23.   23.7  25.   21.8  20.6  21.2  19.1  20.6  15.2   7.    8.1  13.6\n",
      "  20.1  21.8  24.5  23.1  19.7  18.3  21.2  17.5  16.8  22.4  20.6  23.9\n",
      "  22.   11.9]\n",
      "\n",
      "<class 'numpy.ndarray'>\n",
      "\n",
      "506\n"
     ]
    }
   ],
   "source": [
    "print(boston.target)\n",
    "print()\n",
    "print(type(boston.target))\n",
    "print()\n",
    "print(len(boston.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target array is only one dimmension, lined up in order with the with the observations in the data array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're familiar with the input data, we need to split it up for training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target,\n",
    "                                                    train_size=0.75, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 75% of the data as training data, and 25% of the data as testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379 379\n",
      "\n",
      "127 127\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(y_train))\n",
    "print()\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn, as soon as you have `X_train`, `X_test`, `y_train`, and `y_test`, everything else is just a matter of choosing parameters for whichever model you choose. But this should not be trivialized, selecting models and that model's parameters is *very* important. While we will not cover it here, you should always select the model and parameters best suited for your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.857589194232\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "dt_reg = tree.DecisionTreeRegressor(criterion='mse',  # how to measure fit\n",
    "                                    splitter='best',  # or 'random' for random best split\n",
    "                                    max_depth=None,  # how deep tree nodes can go\n",
    "                                    min_samples_split=2,  # samples needed to split node\n",
    "                                    min_samples_leaf=1,  # samples needed for a leaf\n",
    "                                    min_weight_fraction_leaf=0.0,  # weight of samples needed for a node\n",
    "                                    max_features=None,  # number of features to look for when splitting\n",
    "                                    max_leaf_nodes=None,  # max nodes\n",
    "                                    min_impurity_split=1e-07)  # early stopping\n",
    "\n",
    "model = dt_reg.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 634)\t1\n",
      "  (0, 635)\t1\n",
      "  (0, 636)\t1\n",
      "  (0, 637)\t1\n",
      "  (0, 659)\t1\n",
      "  (0, 660)\t1\n",
      "  (0, 661)\t1\n",
      "  (0, 662)\t1\n",
      "  (0, 663)\t1\n",
      "  (0, 664)\t1\n",
      "  (0, 665)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 40)\t1\n",
      "  (1, 42)\t1\n",
      "  (1, 332)\t1\n",
      "  (1, 340)\t1\n",
      "  (1, 341)\t1\n",
      "  (1, 351)\t1\n",
      "  (1, 352)\t1\n",
      "  (1, 353)\t1\n",
      "  (1, 354)\t1\n",
      "  (1, 355)\t1\n",
      "  :\t:\n",
      "  (377, 1)\t1\n",
      "  (377, 387)\t1\n",
      "  (377, 487)\t1\n",
      "  (377, 488)\t1\n",
      "  (377, 489)\t1\n",
      "  (377, 491)\t1\n",
      "  (377, 505)\t1\n",
      "  (377, 525)\t1\n",
      "  (377, 526)\t1\n",
      "  (377, 542)\t1\n",
      "  (377, 543)\t1\n",
      "  (377, 544)\t1\n",
      "  (378, 0)\t1\n",
      "  (378, 1)\t1\n",
      "  (378, 2)\t1\n",
      "  (378, 40)\t1\n",
      "  (378, 42)\t1\n",
      "  (378, 43)\t1\n",
      "  (378, 44)\t1\n",
      "  (378, 46)\t1\n",
      "  (378, 47)\t1\n",
      "  (378, 48)\t1\n",
      "  (378, 49)\t1\n",
      "  (378, 53)\t1\n",
      "  (378, 54)\t1\n"
     ]
    }
   ],
   "source": [
    "print(model.decision_path(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_leaf': 5, 'min_samples_split': 4}\n",
      "0.751487537371\n",
      "0.812122447534\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'min_samples_split': range(2,10),\n",
    "              'min_samples_leaf': range(1,10)}\n",
    "\n",
    "model_r = GridSearchCV(tree.DecisionTreeRegressor(), param_grid)\n",
    "model_r.fit(X_train, y_train)\n",
    "\n",
    "best_index = np.argmax(model_r.cv_results_[\"mean_test_score\"])\n",
    "\n",
    "print(model_r.cv_results_[\"params\"][best_index])\n",
    "print(max(model_r.cv_results_[\"mean_test_score\"]))\n",
    "print(model_r.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
