---
title: "Generalized additive models (GAMs) and Multivariate adaptive regression splines (MARS/EARTH) - rough draft"
author: "Evan Muzzall"
date: "3/17/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##1. Package installation  
We will use the following packages for this example:
```{r}
if (F) {
  install.packages(c("akima", "caret", "devtools", "earth", "gam", "ggplot2", "mgcv", "mlbench", "plotmo")) # run lines 16 and 17 manually if needed
  devtools::install_github("ck37/ck37r")
}

library(akima)
library(caret)
library(ck37r)
library(devtools)
library(gam)
library(ggplot2)
library(mgcv)
library(mlbench)
library(plotmo)
library(earth)
```

##2. Goals
Use the "PimaIndiansDiabetes2" dataset to construct a generalized additive model (GAM) and multivariate additive regression model (MARS, aka EARTH). blood pressure will be the response variable. Missing data will be median-imputed and indicators will be created to document their missingness.  

##3. Preprocess the data
```{r}
# load the dataset
data(PimaIndiansDiabetes2)
?PimaIndiansDiabetes2
data <- PimaIndiansDiabetes2 # give the data a simpler name
str(data)
```

Check for missing data:
```{r}
# check for missing cases
sum(is.na(data)) 

# how much of the data is missing? 
sum(is.na(data)) / (nrow(data)*ncol(data)) # about 9% 
```

Recode the "diabetes" vector to numeric type:
```{r}
data$diabetes <- ifelse(data$diabetes=="pos", 1, 0)
```

Use Chris K's handy median impute function to impute missing values: 
```{r}
# impute and add missingness indicators
result = ck37r::impute_missing_values(data) 

# overwrite "data" with new imputed data frame
data <- result$data 
```

Double check that missing values have been imputed:
```{r}
# no more NA values
sum(is.na(data))

# check that missingness indicators have been added
str(data)
```

##4. Generalized additive models (GAMs)
This semester, MLWG has explored linear, polynomial, and spline regression models using single predictors (March 3) as well as stepwise selection using multiple predictors (Feb 17). Deb also offered an informative take on splines earlier today (Mar 17). Last semester, we talked about improving linear regression models via penalized regression (LASSO and ridge) using multiple predictors (Nov 4).  

When considering multilple predictor variables, another extension of multiple linear regression can be used - generalized additive models.  

Generalized additive models (GAMs) are another extension of multiple linear regression. They are not bound by linear relationships between predictor and response variable and can instead incorporate smoothed, nonlinear relationships. Each relationships is computed and summed (thus making it "additive"). Smoothed splines are not the only constructs used to build GAMs, as they can be built using natural splines, local regression, polynomial regression, etc.  

"Backfitting", or updating the model as each predictor is approximated using penalized likelihood maximization,  comprises the smoothed spline. 

See [Wood's book](https://www.crcpress.com/Generalized-Additive-Models-An-Introduction-with-R/Wood/p/book/9781584884743) for thorough walkthroughs of GAMs in R. 

As always, we also encourage [Introduction to Statistical Learning - Chapter 7](http://www-bcf.usc.edu/~gareth/ISL/) for a nice introductory overview and exercises.  
See [Faraway 2002](https://cran.r-project.org/doc/contrib/Faraway-PRA.pdf) for a great intro to regression and ANOVA

Fit the GAM: 
```{r}
gam1 <- gam(pressure ~ s(glucose) + s(insulin) + s(age) + diabetes,
            family="gaussian",
            method="GCV.Cp",
            data=data)

gam1
# view summary output
gam.check(gam1)

names(gam1)
gam1$aic 
gam1$sig2
```

Play with some basic plotting features
```{r}
plot(gam1, se=T, 
         shade=T, col="black", shade.col="gray80", 
         residuals=F,
         pages=1)
title("gam1")
```

##5. Compare the GAM to other similar GAMs!
Our plots suggest that "glucose" is fairly linear. What if we compare `gam1` to two other GAMs - one that _excludes_ the predictor glucose, and another that _assumes a linear relationship_ of glucose?
```{r}
# model that excludes glucose
gam2 <- gam(pressure ~ s(insulin) + s(age) + diabetes,
            family="gaussian",
            method="GCV.Cp",
            data=data)

plot(gam2, pages=1)

# model that assumes linear glucose
gam3 <- gam(pressure ~ glucose + s(insulin) + s(age) + diabetes,
            family="gaussian",
            method="GCV.Cp",
            data=data)

plot(gam3, pages=1)

anova(gam1, gam2, gam3, test="F") # small p-value suggests that a non-linear function for glucose is preferable?

AIC(gam1, gam2, gam3) # is this a multiple comparison problem? 
BIC(gam1, gam2, gam3)
```

What if we want to identify unhelpful predictors and remove them for better results?
```{r}
table(data$diabetes, I(data$pregnant>14))

gam4 <- gam(pressure ~ s(glucose) + s(insulin) + s(age) + diabetes,
            family="gaussian",
            data=data,
            subset=(diabetes !=0))

plot(gam4, se=TRUE, seWithMean=TRUE, 
         shade=TRUE, col="blue", shade.col="lightgreen", 
         residuals=FALSE,
         pages=1)
title("GAM - adjusted predictors")

AIC(gam1, gam2, gam3, gam4)
```

##6. plotmo
The "plotmo" R package offers a great way to visualize regression splines in three dimensions:
```{r}
plotmo(gam1, all2=TRUE) # show simplfied seWithMean plots AND three dimensional splines for all variable relationships

# non-additive shapes have correlated effects in 3D plane surfaces.

# plot partial dependencies (takes a few minutes)
# plotmo(gam1, all2=TRUE, pmethod = "partdep") 

# faster version of pmethod="partdep"
plotmo(gam1, all2=TRUE, pmethod = "apartdep", 
       caption = "What have I gotten myself in to...") 

# let's play around with a few more parameters! 
plotmo(gam1, all2=TRUE, pt.col = "green3")
plotmo(gam1, all2=TRUE, pt.col = "green3", smooth.col = "red")
plotmo(gam1, all2=TRUE,  
       pt.col = "green3", 
       smooth.col = "red",
       grid.col="gray80")

# return just some of the plots! 
plotmo(gam1, all2=TRUE, degree1 = c(1,2), degree2=0, col="tomato") # show just the first two predictor plots

plotmo(gam1, all2=TRUE, degree1 = 0, degree2 = 1, # return just glucose v. pregnant perspective plot
       caption = "this is called a 'perspective plot'",
       persp.col="orange")
```

See [Wood S. 2006. Generalized additive models: An introduction with R](https://www.amazon.com/Generalized-Additive-Models-Introduction-Statistical/dp/1584884746) for expert explanations.  

["gam" R package](https://cran.r-project.org/web/packages/gam/index.html)

["mgcv" R package](https://cran.r-project.org/web/packages/mgcv/mgcv.pdf)

Also check out [Stephen Milborrow's excellent instructions on the "plotmo" R package](http://www.milbo.org/doc/plotmo-notes.pdf)

##7. Multivariate adaptive regression splines (MARS) and (earth)
Multivariate adaptive regression splines (MARS) are a technique developed by Jerome H. Friedman in 1991 and copyrighted by Salford Systems. Open source implementations are thusly referred to as "earth", but may not be identical to MARS. Also see the ["mda" R package](https://cran.r-project.org/web/packages/mda/index.html) and Friedman papers for specifics.  

earth = Enhanced Adaptive Regression Through Hinges   

These approaches use "surrogate features" (or, models of the models), usually versions of one or two predictors at a time. Each predictor is divided into two groups and each group models the outcome variable for each group. This creates a "piecewise linear model" where each new feature is some proportion of the data. 

Group definitions are provided via linear regression models! Those with the smallest error are used. See [Kuhn and Johnson, 2016:145 ](http://appliedpredictivemodeling.com/) for more information. 

Fit the earth model
```{r}
# fit the model
set.seed(1)
earth1 <- earth(pressure ~ ., data=data, 
                degree=1, nk=5, 
                keepxy=TRUE, nprune=20, nfold=10, ncross=2,
                pmethod="cv", trace=4) 

# view summary output
summary(earth1, details=TRUE)

# view predictor importance
evimp(earth1)

# compute predicted values
earth_pred <- predict(earth1)

# print accuracy
(mse <- mean((data$pressure - earth_pred)^2))
```

Earth plots
```{r}
# plot
# png("earth1.png")
plot(earth1)
# dev.off()
plot(earth1, info=T, type="response", trace=1)
plotmo(earth1, info=T, type="response", trace=1)#, level=.9)

# 3d MARS plots!
# same syntactical rules apply here as well
plotmo(earth1)

plotmo(earth1, all2=TRUE, persp.col="azure")
```

We can also see the ideal number of terms
```{r}
control <- trainControl(method = "repeatedcv",
                        repeats = 1, number = 1)

grid <- expand.grid(.degree = 1, .nprune = 2:25)

earth_best_terms <- train(pressure ~ ., data = data, method = "earth",
tuneGrid= grid)

earth_best_terms
plot(earth_best_terms)
```

TODO: 
- determine best value for nfold
- explore the ncross argument
- plot cross validation results
- collect $R^2$ in different ways
- use cross-validation to select the number of terms
- better discuss partial dependence plots
- include confidence intervals versus prediction intervals
- investigate assumptions of prediction intervals
- include text about interpretaiton of 3D plotmo regression surfaces
- comprehensively discuss limitations

[See Stephen Milborrow's excellent notes on earth here](http://www.milbo.org/doc/earth-notes.pdf) for lots of handy tips and tricks.  

[... and view his notes on variance models in earth here](http://www.milbo.org/doc/earth-varmod.pdf)

["earth" R package](https://cran.r-project.org/web/packages/earth/earth.pdf)

[Friedman 1991 - MARS](https://projecteuclid.org/download/pdf_1/euclid.aos/1176347963)

[Friedman 1993- Fast MARS](https://statistics.stanford.edu/sites/default/files/LCS%20110.pdf) 

