---
title: "Penalized regression in R"
output:
  html_document: default
  html_notebook: default
---
# Data prep

```{r}
library(MASS)
data(Boston)
help(Boston)
str(Boston)
summary(Boston)

# Our outcome is median home value.
outcome = "medv"

# Divide into 80% training, 20% test split.
library(caret)
set.seed(1)
train_index = caret::createDataPartition(Boston[, outcome], p = .8, 
                                  list = F, 
                                  times = 1)

# Glmnet wants the data to be matrices, not data frames.
X_train = as.matrix(Boston[train_index, !names(Boston) == outcome])
X_test = as.matrix(Boston[-train_index, !names(Boston) == outcome])

Y_train = Boston[train_index, outcome]
Y_test = Boston[-train_index, outcome]

dim(X_train)
length(Y_train)

dim(X_test)
length(Y_test)
```


# Lasso

Lasso penalizes coefficients and imposes sparsity, so some coefficients may be shrunk to 0 if they do not appear to be related to the outcome.

```{r}
library(glmnet)
# Fit the lasso to continuous Y
reg = cv.glmnet(X_train, Y_train, family = "gaussian", alpha = 1)

# Look at distribution of penalty term lambda.
plot(reg)

# Plot the underlying glmnet object, showing
# coefficients for differnt lambda values.
plot(reg$glmnet.fit, xvar = "lambda", label = T)

# Lambda with minimum mean-squared error.
reg$lambda.min

# Higher lambda within 1SE of performance of the minimum.
# (the "one standard error" rule from Leo Breiman.)
reg$lambda.1se

# Review coeffients
coef(reg, s = "lambda.1se")

# What about for lambda.min?
coef(reg, s = "lambda.min")

# Predict on test set.
pred = predict(reg, s = reg$lambda.1se, newx = X_test)

# Calculate mean-squared error.
mean((pred - Y_test)^2)
```

# Ridge

Ridge penalizes the coefficients but does not impose sparsity, so no coefficient will ever be 0.

```{r}

# Fit the ridge to continuous Y
# We just change alpha to 0 to get ridge regression.
reg = cv.glmnet(X_train, Y_train, family = "gaussian", alpha = 0)

# Look at distribution of penalty term lambda.
plot(reg)

# Plot the underlying glmnet object, showing
# coefficients for differnt lambda values.
plot(reg$glmnet.fit, xvar = "lambda", label = T)

# Predict on test set.
pred = predict(reg, s = reg$lambda.1se, newx = X_test)

# Calculate mean-squared error.
mean((pred - Y_test)^2)
```

As expected, we do a little worse with ridge compared to lasso.

# Elastic net

```{r}
train_control = trainControl(method = "repeatedCV",
                             number = 10,
                             repeats = 3)

set.seed(1)

# Create a custom tuning grid.
enet_grid = expand.grid(alpha = seq(0, 1, length.out = 5),
                        lambda = 2^seq(-1, -7, length = 5))

# Review the grid.
enet_grid

# To be simpler we could just say e.g. tuneLength = 5.

enet = train(X_train, Y_train, method = "glmnet",
             #tuneLength = 5,
             tuneGrid = enet_grid,
             trControl = train_control)

print(enet)

plot(enet)

enet$bestTune

# Predict on test.
pred = predict(enet, X_test)

# Review performance
mean((pred - Y_test)^2)
```

# References

Intro to Statistical Learning, Chapter 6

[Glmnet vignette by Hastie and Qian](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html) - lots of great code examples