---
title: "Support vector machines in R"
output:
  html_document: default
  html_notebook: default
---

Topics to cover:

* Background
* Polynomial kernels
* Radial basis function (RBF) kernel
* Hyperparameter optimization

Before we dig in, we will install the R packages we'll be using.

**R packages**
```{r}
# List of packages we will use.
packages = c("MASS", "kernlab", "devtools")

# Try to load each package and save the result.
success = sapply(packages, require, character.only = T, quietly = T)

# Check if any packages still need to be installed.
if (sum(!success) > 0) {
  # Install any needed packages.
  install.packages(packages[!success])
  
  # Load the newly installed packages.
  sapply(packages[!success], require, character.only = T, quietly = T)
}

github_packages = c(
  # Chris's tools package for plotting the SuperLearner.
  "ck37/ck37r",
  # Use more up-to-date SuperLearner from github.
  "ecpolley/SuperLearner")

devtools::install_github(github_packages)

# Clean up variables.
rm(packages, success, github_packages)
```

# Background

Deb has done a great job covering the background & theory for SVM in the python materials. See also Intro to Statistical Learning Chapter 9, Applied Predictive Modeling sections 7.3, 13.4, 13.7.4, or pretty much any other machine learning textbook.

Motivational quote: "SVMs not only have a more solid foundation than artificial neural nets, but are able to serve as a replacement for neural nets." - Learning with Kernels, 2002.

# Software packages

There are many R packages that implement support vector machines. `e1071` is one of the oldest, but we will use `kernlab` because it implements more kernels and has a broader set of features. `klaR` and `svmPath` are other options; `svmPath` is particularly designed to be fast.

# Data preparation

```{r}
data(Boston, package = "MASS")

# Remove our outcome variable from the covariate list.
X_df = Boston[, -14]

# Convert X from a dataframe to a matrix.
X_mat = model.matrix(~ .,  data = X_df)

# Notice the extra intercept column added by model.matrix.
colnames(X_mat)
                 
#  Remove extra intercept term.
X_mat = X_mat[, -1]

# Regression (continuous) version of our outcome variable.
Y_reg = Boston$medv

# Review outcome distribution.
summary(Y_reg)

# Classification (binary) version of our outcome variable.
Y_class = as.factor(as.numeric(Boston$medv > 23))

# Review outcome distribution.
table(Y_class)
prop.table(table(Y_class))

```

Also note that the SVM algorithms will internally center (mean 0) and scale all variables so that they can be compared to each other, as we did with K-nearest neighbors (lasso also does this).

# Polynomial kernels

Polynomial kernels are the simplest kernel you would want to use with SVM. You can do a linear kernel (i.e. no polynomial expansion) but that's essentially equivalent to OLS and is only done if the data is so big that a better kernel is computationlly infeasible. 

```{r errors=T}
library(kernlab)

# Regression version.
# X should be a matrix, not a dataframe.
fit = ksvm(x = X_mat, y = Y_reg, kernel = "polydot", kpar = list(degree = 3))
fit

# Compare to using a dataframe rather than a matrix.
# The error message is particularly unhelpful.
fit = ksvm(x = X, y = Y_reg, kernel = "polydot")

# Classification version.
# Y should be a factor so that classification is run automatically.
fit = ksvm(x = X_mat, y = Y_class, kernel = "polydot",  kpar = list(degree = 3))
fit

```

# Radial basis function kernels

RBF is the best kernel if you have to pick one.

```{r}

# Regression version.
fit = ksvm(x = X_mat, y = Y_reg, kernel = "rbfdot")
fit

# Classification
fit = ksvm(x = X_mat, y = Y_class, kernel = "rbfdot")
fit

```

# Hyperparameter optimization

There are two hyperparameters that can potentially have major effects on the performance of SVM: the amount of regularization (called "C" often), and the bandwidth (scale parameter often called gamma or sigma).

The regularization parameter C is the non-negative error budget for the number of misclassifications allowed, critical to establish bias-variance trade-off. When C is small we want low bias but high variance, and the reverse when C is large. Thorough C grid points: $C \in \{2^{−5},2^{−3},...,2^{15}\}$

The scale parameter Gamma ($\gamma$) aka Sigma ($\sigma$) is effectively the inverse bandwidth of the SVM kernel. So a large gamma/sigma corresponds to a wide bandwidth used to calculate proximity, meaning that a wider range of points are incorporated.  When gamma/sigma is small only very nearby observations are used. Thorough grid points: $\gamma or \sigma \in \{2^{-15}, 2^{-13}, ..., 2^{3}\}$. Notably, a good initial guess is generated by kernlab's `sigest` function, which may allow one to skip optimizing this hyperparameter.

```{r}

# Make sure you have the latest version from github.
library(SuperLearner)

tuning_list = list(
  # Try different kernels.
  # Unfortunately we cannot currently customize the degree for the polynomial kernel when
  # using create.Learner() - this will be fixed. We could make the functions manually though.
  kernel = c("polydot", "rbfdot", "laplacedot"),
  # Regularization parameter, could be 2^-5 to 2^15.
  C = 2^c(-4, -2, 0, 2, 4, 6, 8)
)

# Review the C values we are testing.
tuning_list$C

svm_learners = create.Learner("SL.ksvm", detailed_names = T,
                              tune = tuning_list)

# Review the learners that were created.
svm_learners$names

sl_lib = c("SL.mean", "SL.glm", "SL.ksvm", svm_learners$names)

set.seed(1)

Y_num = as.numeric(Y_class) - 1
table(Y_num, Y_class)

# Currently displays some extra output unfortunately.
result = SuperLearner(Y = Y_num, X = Boston[, -14], family = "binomial",
                      SL.library = sl_lib)
result

# Use plot.SuperLearner() from here.
library(ck37r)

# Plot the results. Use CV.SuperLearner to also plot the SL performance.
# We need to pass in the outcome variable for this plot.
plot(result, Y_num)
```

# Other notes

SVM has to compute the kernel function for every pair of observations, so it is not ideal when you have many observations. It's best with a reasonable number of observations but possibly a large number of covariates.

# Further reading on SVMs

Scholkopf, B., & Smola, A. J. (2002). Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT press.