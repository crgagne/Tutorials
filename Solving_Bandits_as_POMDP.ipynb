{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimally Solving the Bandit Problem\n",
    "\n",
    "- since the bandit problem has a small finite horizon, you can work backwards optimally solve the problem. (otherwise you'd have to use the gittens index or something to optimally balance exploration and exploitation) \n",
    "- but you haven't seen any options? You can enumerate each possibility at each stage? A branching tree and then back-track. So the first stage as 8 possible outcomes. 4 choices, 2 outcomes each. Or do we want a (state,action) pair. \n",
    "- This is similar to how you would solve with dynamic programming. Wait is this dynamic programming on belief state POMDP? I think it might be. Ok. step 1. enumerate the tree (but for state or actions). Just the possibilities, or the beliefs as well. step 2: start at the end, which branch? and say ok I chose bandit 1.\n",
    "\n",
    "- state's might be belief states. S = [2,3; 1,2; 0,1] after six trials. \n",
    "- the we also have the expected total reward to be followed on the remaining trials. u(S)?\n",
    "    - s+A / s+f+a+b * u(k+1, where s+1)  + f+B / s+f+a+b * u(k+1, where f+1)\n",
    "- you then choose the max. of this. \n",
    "\n",
    "\n",
    "\n",
    "Dynamic programming: \n",
    "\n",
    "In the value iteration, you loop through the states (in no particular order) and increment the value function for each state based on next set of states weighted by their probabilities. \n",
    "\n",
    "In this case, we can start at the end state instead of sampling states randomly. Therefore, we only need to do one pass through the states. \n",
    "\n",
    "We update the value function for each state, based on the probability of going to the next state, which is given by what we've observed in each state, and then update it's value based on what we find there. \n",
    "\n",
    "Then to actually do the task, we simply solve forward by choosing the best action. \n",
    "\n",
    "We need an efficient and intuitive state representation and implementation in code. \n",
    "What's important about the state: \n",
    "- number of heads, tails for each option in order .. option1, option2 etc. (I'll try this first with just a few states). \n",
    "- or number of heads, tails for best option, second best option etc. \n",
    "\n",
    "\n",
    "\n",
    "states tensor = number heads X number tails X option # not all of this is possible at once? .. nevermind. \n",
    "\n",
    "\n",
    "using the full bayesian solution is called solving the bandit process - gittins provides a simpler calculation. https://people.eecs.berkeley.edu/~avivt/BRLS_journal.pdf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandit_dp(K=2,N=5,alpha=1.0,beta=1.0):\n",
    "\n",
    "    # value function, indexed by state \n",
    "    v = np.empty((np.repeat(N,K*2)))*np.nan   # ***might need to change this to Q values # \n",
    "    q = np.empty((np.append(np.repeat(N,K*2),np.array([K]))))*np.nan   # ***might need to change this to Q values # \n",
    "    \n",
    "    # state representations = [s1,f1,s2,f2,...sk,fk]\n",
    "    # some states are impossible, but they will stay nan's. \n",
    "\n",
    "    for trial in np.arange(N)[::-1]:\n",
    "        #print('')\n",
    "        print('trial {0}').format(trial)\n",
    "\n",
    "        # enumerate each possible state\n",
    "        for state,__ in np.ndenumerate(v):\n",
    "            state = np.array(state) \n",
    "\n",
    "            # only consider states for this trial with \n",
    "            if np.sum(state)==(trial): \n",
    "                #print('state:{0}').format(state)\n",
    "                for bandit in range(K):\n",
    "                    #\n",
    "                    #print('bandit: {0}').format(bandit)\n",
    "                    s = np.float(state[0+bandit*K])\n",
    "                    f = np.float(state[1+bandit*K])\n",
    "\n",
    "                    if trial==N-1: # last trials\n",
    "                        # final state value \n",
    "                        vnext_s=0.0\n",
    "                        vnext_f=0.0\n",
    "                    else:\n",
    "                        # increment the success for that bandit to get next state\n",
    "                        snext_s = state.copy()\n",
    "                        snext_s[0+bandit*K]+=1 \n",
    "                        vnext_s = v[tuple(snext_s)]\n",
    "\n",
    "                        # increment the failure for that bandit to get next state\n",
    "                        snext_f = state.copy()\n",
    "                        snext_f[1+bandit*K]+=1  \n",
    "                        vnext_f = v[tuple(snext_f)]\n",
    "\n",
    "#                         print('snext_success: {0}').format(snext_s)\n",
    "#                         print('snext_fail: {0}').format(snext_f)\n",
    "#                     print('qnext_fail: {0}').format(qnext_f)\n",
    "#                     print('qnext_success: {0}').format(qnext_s)\n",
    "                    es = (s+alpha)/(s+f+alpha+beta)\n",
    "                    ef = (f+beta)/(s+f+alpha+beta)\n",
    "                    qval = es*(vnext_s+1.0) + ef*vnext_f\n",
    "#                     print('weight s: {0}').format(es)\n",
    "#                     print('weight f: {0}').format(ef)\n",
    "#                     print('q: {0}').format(qval)\n",
    "                    q[tuple(np.append(state,bandit))]=qval\n",
    "\n",
    "                # take max value over actions in this state\n",
    "                v[tuple(state)]=np.max(q[tuple(state)])\n",
    "    return(v,q)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial 3\n",
      "trial 2\n",
      "trial 1\n",
      "trial 0\n",
      "\n",
      "finished results:\n",
      "first trial values\n",
      "[ 1.36111111  1.52777778]\n",
      "[ 2.02777778  1.86111111]\n",
      "\n",
      "2nd trial\n",
      "[ 0.72222222  0.72222222]\n",
      "[ 1.38888889  1.38888889]\n",
      "[ 1.33333333  1.        ]\n",
      "[ 1.5   1.25]\n",
      "[ 0.75  1.  ]\n",
      "\n",
      "3rd trial\n",
      "[ 0.25        0.33333333]\n",
      "[ 0.75        0.66666667]\n",
      "[ 0.75        0.33333333]\n",
      "[ 0.6  0.5]\n",
      "[ 0.8  0.5]\n"
     ]
    }
   ],
   "source": [
    "## simple problem with 2 bandits, 3 trials. \n",
    "v,q = bandit_dp(K=2,N=4,alpha=1.0,beta=1.0)\n",
    "\n",
    "print('')\n",
    "print('finished results:')\n",
    "print('first trial values')\n",
    "print(q[(0,1,0,0)]) # one failure, you're expected value is still 0.5, because you choose that one \n",
    "print(q[(1,0,0,0)]) # one success, you're expected value is 0.666, because you choose the next one. \n",
    "\n",
    "\n",
    "print('')\n",
    "print('2nd trial')\n",
    "print(q[(0,1,0,1)]) # one failure for each. and you have one choice left, what's you're expected values? \n",
    "print(q[(1,0,1,0)]) # one success for each: \n",
    "print(q[(1,0,0,1)]) # one success, one failure: notice how it's the same as 2 success, because you are assumed to choose the best. \n",
    "print(q[(2,0,0,0)]) # two successes\n",
    "print(q[(0,2,0,0)]) # two failes\n",
    "\n",
    "\n",
    "print('')\n",
    "print('3rd trial')\n",
    "print(q[(0,2,0,1)]) # two failures, one failure and you have one choice left, what's you're expected values? \n",
    "print(q[(2,0,1,0)]) # two successes, one success\n",
    "print(q[(2,0,0,1)]) # two successes, one failure\n",
    "print(q[(2,1,0,0)]) # two/1\n",
    "print(q[(3,0,0,0)]) # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solving game...\n",
      "trial 19\n",
      "trial 18\n",
      "trial 17\n",
      "trial 16\n",
      "trial 15\n",
      "trial 14\n",
      "trial 13\n",
      "trial 12\n",
      "trial 11\n",
      "trial 10\n",
      "trial 9\n",
      "trial 8\n",
      "trial 7\n",
      "trial 6\n",
      "trial 5\n",
      "trial 4\n",
      "trial 3\n",
      "trial 2\n",
      "trial 1\n",
      "trial 0\n",
      "playing game...\n",
      "trials: 0\n",
      "state: [0 0 0 0]\n",
      "q: [ 12.43126491  12.43126491]\n",
      "action: 0\n",
      "outcome: 1\n",
      "\n",
      "trials: 1\n",
      "state: [1 0 0 0]\n",
      "q: [ 13.51116571  13.39216399]\n",
      "action: 0\n",
      "outcome: 1\n",
      "\n",
      "trials: 2\n",
      "state: [2 0 0 0]\n",
      "q: [ 13.82476132  13.64625874]\n",
      "action: 0\n",
      "outcome: 1\n",
      "\n",
      "trials: 3\n",
      "state: [3 0 0 0]\n",
      "q: [ 13.7265145   13.48851106]\n",
      "action: 0\n",
      "outcome: 1\n",
      "\n",
      "trials: 4\n",
      "state: [4 0 0 0]\n",
      "q: [ 13.38162552  13.0904842 ]\n",
      "action: 0\n",
      "outcome: 1\n",
      "\n",
      "trials: 5\n",
      "state: [5 0 0 0]\n",
      "q: [ 12.87383719  12.54202475]\n",
      "action: 0\n",
      "outcome: 0\n",
      "\n",
      "trials: 6\n",
      "state: [5 1 0 0]\n",
      "q: [ 10.58577281  10.42925321]\n",
      "action: 0\n",
      "outcome: 0\n",
      "\n",
      "trials: 7\n",
      "state: [5 2 0 0]\n",
      "q: [ 8.90847813  8.9128011 ]\n",
      "action: 1\n",
      "outcome: 1\n",
      "\n",
      "trials: 8\n",
      "state: [5 2 1 0]\n",
      "q: [ 8.7524631   8.82395189]\n",
      "action: 1\n",
      "outcome: 1\n",
      "\n",
      "trials: 9\n",
      "state: [5 2 2 0]\n",
      "q: [ 8.43423035  8.54146354]\n",
      "action: 1\n",
      "outcome: 1\n",
      "\n",
      "trials: 10\n",
      "state: [5 2 3 0]\n",
      "q: [ 7.95883542  8.101813  ]\n",
      "action: 1\n",
      "outcome: 1\n",
      "\n",
      "trials: 11\n",
      "state: [5 2 4 0]\n",
      "q: [ 7.36418321  7.53344173]\n",
      "action: 1\n",
      "outcome: 1\n",
      "\n",
      "trials: 12\n",
      "state: [5 2 5 0]\n",
      "q: [ 6.67568351  6.86665254]\n",
      "action: 1\n",
      "outcome: 1\n",
      "\n",
      "trials: 13\n",
      "state: [5 2 6 0]\n",
      "q: [ 5.91872066  6.12735314]\n",
      "action: 1\n",
      "outcome: 0\n",
      "\n",
      "trials: 14\n",
      "state: [5 2 6 1]\n",
      "q: [ 4.56940971  4.68272751]\n",
      "action: 1\n",
      "outcome: 1\n",
      "\n",
      "trials: 15\n",
      "state: [5 2 7 1]\n",
      "q: [ 3.86979445  4.00328762]\n",
      "action: 1\n",
      "outcome: 1\n",
      "\n",
      "trials: 16\n",
      "state: [5 2 8 1]\n",
      "q: [ 3.12144522  3.27306027]\n",
      "action: 1\n",
      "outcome: 1\n",
      "\n",
      "trials: 17\n",
      "state: [5 2 9 1]\n",
      "q: [ 2.33333333  2.5       ]\n",
      "action: 1\n",
      "outcome: 1\n",
      "\n",
      "trials: 18\n",
      "state: [ 5  2 10  1]\n",
      "q: [ 1.51282051  1.69230769]\n",
      "action: 1\n",
      "outcome: 1\n",
      "\n",
      "trials: 19\n",
      "state: [ 5  2 11  1]\n",
      "q: [ 0.66666667  0.85714286]\n",
      "action: 1\n",
      "outcome: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### play bandit game #### \n",
    "\n",
    "# specify game \n",
    "K = 2\n",
    "N = 20\n",
    "\n",
    "# specify starting belief\n",
    "alpha=1.0\n",
    "beta=1.0\n",
    "\n",
    "# solve for the optimal value function for each possible state of the game \n",
    "print('solving game...')\n",
    "v,q = bandit_dp(K,N,alpha,beta)\n",
    "\n",
    "# specify starting state, and bandit probabilities \n",
    "s0 = np.array([0,0,0,0])\n",
    "p = [0.3,0.7]\n",
    "\n",
    "#\n",
    "print('playing game...')\n",
    "\n",
    "for trial in np.arange(N): #[0,1,2]: #np.arange(N):\n",
    "    print('trials: {0}').format(trial)\n",
    "    if trial==0:\n",
    "        state = s0\n",
    "    \n",
    "    # choose action # \n",
    "    qvals = q[tuple(state)]\n",
    "    bandit = np.argmax(qvals)\n",
    "    \n",
    "    # observe response \n",
    "    outcome = scipy.stats.bernoulli(p[bandit]).rvs() # different for different bandits # \n",
    "    \n",
    "    print('state: {0}').format(state)\n",
    "    print('q: {0}').format(qvals)\n",
    "    print('action: {0}').format(bandit)\n",
    "    print('outcome: {0}').format(outcome)\n",
    "    print('')\n",
    "    \n",
    "\n",
    "    # increment state\n",
    "    state[(1-outcome)+bandit*K]+=1\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1rc1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "65px",
    "width": "253px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
